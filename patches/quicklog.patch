diff -rupN linux-3.10.0-1160.49.1.el7.x86_64/kernel/aes-mac1.h aes/kernel/aes-mac1.h
--- linux-3.10.0-1160.49.1.el7.x86_64/kernel/aes-mac1.h	1969-12-31 19:00:00.000000000 -0500
+++ aes/kernel/aes-mac1.h	2022-08-24 21:51:49.902422711 -0400
@@ -0,0 +1,24 @@
+/*
+*/
+
+#ifndef AES_MAC1_H
+#define AES_MAC1_H
+
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+  typedef __m128i block;
+  typedef struct {block rd_key[11]; } AES_KEY;
+ 
+
+  /* Streaming API */
+  void AES_128_Key_Expansion(const unsigned char *userkey, void *key);  
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif
+
diff -rupN linux-3.10.0-1160.49.1.el7.x86_64/kernel/audit.c aes/kernel/audit.c
--- linux-3.10.0-1160.49.1.el7.x86_64/kernel/audit.c	2021-11-09 08:29:30.000000000 -0500
+++ aes/kernel/audit.c	2022-08-24 21:50:34.221345342 -0400
@@ -66,6 +66,19 @@
 
 #include "audit.h"
 
+/* QuickLog  integrity */
+#include <linux/random.h>
+#include <linux/types.h>
+#include <asm/i387.h>
+
+#define  _MM_MALLOC_H_INCLUDED
+#include <x86intrin.h>
+#undef _MM_MALLOC_H_INCLUDED
+
+#include "aes-mac1.h"
+//---------------------------------------------------------------------------------------
+
+
 /* No auditing will take place until audit_initialized == AUDIT_INITIALIZED.
  * (Initialization happens after skb_init is called.) */
 #define AUDIT_DISABLED		-1
@@ -179,6 +192,402 @@ struct audit_reply {
 	struct sk_buff *skb;
 };
 
+
+//QuickLog  integrity
+//---------------------------------------------------------------------------------------
+
+//static DEFINE_SPINLOCK(lock_set_logging);
+#if 0
+typedef __m128i block;
+typedef struct {block rd_key[11]; } AES_KEY;
+#endif 
+
+const static unsigned char aeskey[16] = {0};
+static AES_KEY const_aeskey;
+static block current_key, current_state;
+static __u64 q_tag[2]={0};
+
+/* Some helper functions */
+#define rnds 10 //AES rounds
+#define xor_block(x,y)        _mm_xor_si128(x,y)
+#define zero_block()          _mm_setzero_si128()
+/*Load 2-byte counter following 14-byte log data*/
+#define gen_logging_blk(log,ctr) _mm_insert_epi16(_mm_loadu_si128(log), ctr, 0)
+
+// Sources:
+// Dead Store Elimination (Still) Considered Harmful, USENIX 2017
+// https://compsec.sysnet.ucsd.edu/secure_memzero.h
+//
+#define EXPAND_ASSIST(v1,v2,v3,v4,shuff_const,aes_const)                    \
+  do{                                                                       \
+    v2 = _mm_aeskeygenassist_si128(v4,aes_const);                           \
+    v3 = _mm_castps_si128(_mm_shuffle_ps(_mm_castsi128_ps(v3),              \
+                                         _mm_castsi128_ps(v1), 16));        \
+    v1 = _mm_xor_si128(v1,v3);                                              \
+    v3 = _mm_castps_si128(_mm_shuffle_ps(_mm_castsi128_ps(v3),              \
+                                         _mm_castsi128_ps(v1), 140));       \
+    v1 = _mm_xor_si128(v1,v3);                                              \
+    v2 = _mm_shuffle_epi32(v2,shuff_const);                                 \
+    v1 = _mm_xor_si128(v1,v2);                                              \
+  } while(0)
+
+
+void AES_128_Key_Expansion(const unsigned char *userkey, void *key)
+{
+    __m128i x0,x1,x2;
+    __m128i *kp = (__m128i *)key;
+    kp[0] = x0 = _mm_loadu_si128((__m128i*)userkey);
+    x2 = zero_block();
+    EXPAND_ASSIST(x0,x1,x2,x0,255,1);   kp[1]  = x0;
+    EXPAND_ASSIST(x0,x1,x2,x0,255,2);   kp[2]  = x0;
+    EXPAND_ASSIST(x0,x1,x2,x0,255,4);   kp[3]  = x0;
+    EXPAND_ASSIST(x0,x1,x2,x0,255,8);   kp[4]  = x0;
+    EXPAND_ASSIST(x0,x1,x2,x0,255,16);  kp[5]  = x0;
+    EXPAND_ASSIST(x0,x1,x2,x0,255,32);  kp[6]  = x0;
+    EXPAND_ASSIST(x0,x1,x2,x0,255,64);  kp[7]  = x0;
+    EXPAND_ASSIST(x0,x1,x2,x0,255,128); kp[8]  = x0;
+    EXPAND_ASSIST(x0,x1,x2,x0,255,27);  kp[9]  = x0;
+    EXPAND_ASSIST(x0,x1,x2,x0,255,54);  kp[10] = x0;
+}
+#undef EXPAND_ASSIST
+
+
+//-------------------------------------------------------------------
+
+#define gen_7_blks(cipher_blks,log_msg,counter)                            \
+    do{                                                                    \
+		cipher_blks[1]  = gen_logging_blk((block*)(log_msg+12),counter+2); \
+		cipher_blks[2]  = gen_logging_blk((block*)(log_msg+26),counter+3); \
+		cipher_blks[3]  = gen_logging_blk((block*)(log_msg+40),counter+4); \
+		cipher_blks[4]  = gen_logging_blk((block*)(log_msg+54),counter+5); \
+		cipher_blks[5]  = gen_logging_blk((block*)(log_msg+68),counter+6); \
+		cipher_blks[6]  = gen_logging_blk((block*)(log_msg+82),counter+7); \
+		cipher_blks[7]  = gen_logging_blk((block*)(log_msg+96),counter+8); \
+	} while(0)
+
+
+//AES pre-round 8 blocks
+#define prernd_8(cipher_blks, key)                   \
+	do{                                                  \
+		cipher_blks[0] = xor_block(cipher_blks[0], key); \
+		cipher_blks[1] = xor_block(cipher_blks[1], key); \
+		cipher_blks[2] = xor_block(cipher_blks[2], key); \
+		cipher_blks[3] = xor_block(cipher_blks[3], key); \
+		cipher_blks[4] = xor_block(cipher_blks[4], key); \
+		cipher_blks[5] = xor_block(cipher_blks[5], key); \
+		cipher_blks[6] = xor_block(cipher_blks[6], key); \
+		cipher_blks[7] = xor_block(cipher_blks[7], key); \
+	} while(0)
+	
+//AES Pre-round 4 blocks
+#define prernd_4(cipher_blks,sign_keys) 			       \
+  do{                                                      \
+    cipher_blks[0] = xor_block(cipher_blks[0], sign_keys); \
+    cipher_blks[1] = xor_block(cipher_blks[1], sign_keys); \
+    cipher_blks[2] = xor_block(cipher_blks[2], sign_keys); \
+    cipher_blks[3] = xor_block(cipher_blks[3], sign_keys); \
+  } while(0)
+
+
+//XOR 8 cipher blocks
+#define tag_8_xor(tag_blks,cipher_blks)                                                                       \
+  do{                                                                                                              \
+	tag_blks[0] =xor_block(xor_block(cipher_blks[0], cipher_blks[1]), xor_block(cipher_blks[2], cipher_blks[3]));  \
+	tag_blks[1] =xor_block(xor_block(cipher_blks[4], cipher_blks[5]), xor_block(cipher_blks[6], cipher_blks[7]));  \
+	tag_blks[2] =xor_block(tag_blks[2], xor_block(tag_blks[0], tag_blks[1]));                                      \
+  } while(0)
+
+
+//one block
+#define aes_single(cipher_blks, sched)                               \
+	do{                                                              \
+		cipher_blks[0] = _mm_aesenc_si128(cipher_blks[0], sched[1]); \
+		cipher_blks[0] = _mm_aesenc_si128(cipher_blks[0], sched[2]); \
+		cipher_blks[0] = _mm_aesenc_si128(cipher_blks[0], sched[3]); \
+		cipher_blks[0] = _mm_aesenc_si128(cipher_blks[0], sched[4]); \
+		cipher_blks[0] = _mm_aesenc_si128(cipher_blks[0], sched[5]); \
+		cipher_blks[0] = _mm_aesenc_si128(cipher_blks[0], sched[6]); \
+		cipher_blks[0] = _mm_aesenc_si128(cipher_blks[0], sched[7]); \
+		cipher_blks[0] = _mm_aesenc_si128(cipher_blks[0], sched[8]); \
+		cipher_blks[0] = _mm_aesenc_si128(cipher_blks[0], sched[9]); \
+		cipher_blks[0] =_mm_aesenclast_si128(cipher_blks[0], sched[10]);\
+	} while(0)
+
+
+//two blocks
+#define enc_2(cipher_blks, key)                                 \
+	do{                                                         \
+		cipher_blks[0] = _mm_aesenc_si128(cipher_blks[0], key); \
+		cipher_blks[1] = _mm_aesenc_si128(cipher_blks[1], key); \
+  	}while(0)
+
+
+
+#define AES_ECB_2(cipher_blks, sched)   \
+	do{     \
+		enc_2(cipher_blks, sched[1]);              \
+		enc_2(cipher_blks, sched[2]);              \
+		enc_2(cipher_blks, sched[3]);              \
+		enc_2(cipher_blks, sched[4]);              \
+		enc_2(cipher_blks, sched[5]);              \
+		enc_2(cipher_blks, sched[6]);              \
+		enc_2(cipher_blks, sched[7]);              \
+		enc_2(cipher_blks, sched[8]);              \
+		enc_2(cipher_blks, sched[9]);              \
+		cipher_blks[0] =_mm_aesenclast_si128(cipher_blks[0], sched[10]); \
+		cipher_blks[1] =_mm_aesenclast_si128(cipher_blks[1], sched[10]); \
+	}while (0)
+
+//three blocks
+#define enc_3(cipher_blks, key)                                 \
+	do{                                                         \
+		cipher_blks[0] = _mm_aesenc_si128(cipher_blks[0], key); \
+		cipher_blks[1] = _mm_aesenc_si128(cipher_blks[1], key); \
+		cipher_blks[2] = _mm_aesenc_si128(cipher_blks[2], key); \
+  	}while(0)
+
+
+#define AES_ECB_3(cipher_blks, sched)    \
+	do{                                  \
+		enc_3(cipher_blks, sched[1]);              \
+		enc_3(cipher_blks, sched[2]);              \
+		enc_3(cipher_blks, sched[3]);              \
+		enc_3(cipher_blks, sched[4]);              \
+		enc_3(cipher_blks, sched[5]);              \
+		enc_3(cipher_blks, sched[6]);              \
+		enc_3(cipher_blks, sched[7]);              \
+		enc_3(cipher_blks, sched[8]);              \
+		enc_3(cipher_blks, sched[9]);              \
+		cipher_blks[0] =_mm_aesenclast_si128(cipher_blks[0], sched[10]); \
+		cipher_blks[1] =_mm_aesenclast_si128(cipher_blks[1], sched[10]); \
+		cipher_blks[3] =_mm_aesenclast_si128(cipher_blks[1], sched[10]); \
+	}while (0)
+
+
+//four blocks
+#define enc_4(cipher_blks, key)                                 \
+	do{                                                         \
+		cipher_blks[0] = _mm_aesenc_si128(cipher_blks[0], key); \
+		cipher_blks[1] = _mm_aesenc_si128(cipher_blks[1], key); \
+		cipher_blks[2] = _mm_aesenc_si128(cipher_blks[2], key); \
+		cipher_blks[3] = _mm_aesenc_si128(cipher_blks[3], key); \
+  	}while(0)
+
+
+#define AES_ECB_4(cipher_blks, sched, sign_keys)   \
+	do{                                        	   \
+		prernd_4(cipher_blks,sign_keys);           \
+		enc_4(cipher_blks, sched[1]);              \
+		enc_4(cipher_blks, sched[2]);              \
+		enc_4(cipher_blks, sched[3]);              \
+		enc_4(cipher_blks, sched[4]);              \
+		enc_4(cipher_blks, sched[5]);              \
+		enc_4(cipher_blks, sched[6]);              \
+		enc_4(cipher_blks, sched[7]);              \
+		enc_4(cipher_blks, sched[8]);              \
+		enc_4(cipher_blks, sched[9]);              \
+		cipher_blks[0] =_mm_aesenclast_si128(cipher_blks[0], sched[10]); \
+		cipher_blks[1] =_mm_aesenclast_si128(cipher_blks[1], sched[10]); \
+		cipher_blks[2] =_mm_aesenclast_si128(cipher_blks[2], sched[10]); \
+		cipher_blks[3] =_mm_aesenclast_si128(cipher_blks[3], sched[10]); \
+	}while (0)
+
+
+//eight blocks
+#define enc_8(cipher_blks, key)                                 \
+	do{                                                         \
+		cipher_blks[0] = _mm_aesenc_si128(cipher_blks[0], key); \
+		cipher_blks[1] = _mm_aesenc_si128(cipher_blks[1], key); \
+		cipher_blks[2] = _mm_aesenc_si128(cipher_blks[2], key); \
+		cipher_blks[3] = _mm_aesenc_si128(cipher_blks[3], key); \
+		cipher_blks[4] = _mm_aesenc_si128(cipher_blks[4], key); \
+		cipher_blks[5] = _mm_aesenc_si128(cipher_blks[5], key); \
+		cipher_blks[6] = _mm_aesenc_si128(cipher_blks[6], key); \
+		cipher_blks[7] = _mm_aesenc_si128(cipher_blks[7], key); \
+  	}while(0)
+
+#define AES_ECB_8(cipher_blks, sched, sign_keys)   \
+	do{                                        	   \
+		prernd_8(cipher_blks,sign_keys);           \
+		enc_8(cipher_blks, sched[1]);              \
+		enc_8(cipher_blks, sched[2]);              \
+		enc_8(cipher_blks, sched[3]);              \
+		enc_8(cipher_blks, sched[4]);              \
+		enc_8(cipher_blks, sched[5]);              \
+		enc_8(cipher_blks, sched[6]);              \
+		enc_8(cipher_blks, sched[7]);              \
+		enc_8(cipher_blks, sched[8]);              \
+		enc_8(cipher_blks, sched[9]);              \
+		cipher_blks[0] =_mm_aesenclast_si128(cipher_blks[0], sched[10]); \
+		cipher_blks[1] =_mm_aesenclast_si128(cipher_blks[1], sched[10]); \
+		cipher_blks[2] =_mm_aesenclast_si128(cipher_blks[2], sched[10]); \
+		cipher_blks[3] =_mm_aesenclast_si128(cipher_blks[3], sched[10]); \
+		cipher_blks[4] =_mm_aesenclast_si128(cipher_blks[4], sched[10]); \
+		cipher_blks[5] =_mm_aesenclast_si128(cipher_blks[5], sched[10]); \
+		cipher_blks[6] =_mm_aesenclast_si128(cipher_blks[6], sched[10]); \
+		cipher_blks[7] =_mm_aesenclast_si128(cipher_blks[7], sched[10]); \
+	}while (0)
+
+
+static void cmpt_4_blks(block *cipher_blks, uint16_t counter, const char *log_msg, block *sched, block sign_keys)
+{
+	if(counter){		
+		cipher_blks[0]  = gen_logging_blk((block*)(log_msg),counter+1); 
+	}else{//contains the first block
+		cipher_blks[0]  = _mm_srli_si128(_mm_loadu_si128((block*)log_msg), 2);
+		cipher_blks[0]  = _mm_insert_epi16(cipher_blks[0], counter+1, 0);
+	}
+	cipher_blks[1]  = gen_logging_blk((block*)(log_msg+12),counter+2); 
+	cipher_blks[2]  = gen_logging_blk((block*)(log_msg+26),counter+3); 
+	cipher_blks[3]  = gen_logging_blk((block*)(log_msg+40),counter+4); 
+	AES_ECB_4(cipher_blks, sched, sign_keys);
+}
+
+
+static void cmpt_2_blks(block *cipher_blks, uint16_t counter, const char *log_msg, block *sched, block sign_keys)
+{   	
+	if(counter){		
+		cipher_blks[0]  = gen_logging_blk((block*)(log_msg),counter+1); //Not the first block
+	}else{//contains the first block
+		cipher_blks[0]  = _mm_srli_si128(_mm_loadu_si128((block*)log_msg), 2);
+		cipher_blks[0]  = _mm_insert_epi16(cipher_blks[0], counter+1, 0);
+	}
+	cipher_blks[1]  = gen_logging_blk((block*)(log_msg+12),counter+2); 
+	/*AES Preround */
+	cipher_blks[0] =_mm_xor_si128(cipher_blks[0], sign_keys);
+	cipher_blks[1] =_mm_xor_si128(cipher_blks[1], sign_keys);
+	AES_ECB_2(cipher_blks, sched);
+}
+
+
+static void cmpt_a_blk(block* cipher_blk, uint16_t counter, const char *log_msg, block *sched, block sign_keys)
+{	
+	if(counter){
+		*cipher_blk = _mm_loadu_si128((block*)log_msg);//Not the first block
+	}else{//it is the first block
+		*cipher_blk = _mm_srli_si128(_mm_loadu_si128((block*)log_msg), 2);//the first block
+	}
+	*cipher_blk = _mm_insert_epi16(*cipher_blk, counter+1, 0);
+	/*AES Preround */
+	*cipher_blk =_mm_xor_si128(*cipher_blk, sign_keys);
+	aes_single(cipher_blk, sched);
+}
+//end of helper functions
+
+/**  
+* MAC, signing a log message and updating the signing-key & state
+* Input @log_msg: a log data, 
+        @msg_len: data size
+* Computing block format: a block(16 bytes) contains "<i>||M_i",  
+*                         2 bytes counter(<i>) and 14 bytes log data(M_i)
+**/
+static void mac_core( char *log_msg, size_t msg_len)
+{
+	uint16_t i, remaining , counter = 0;  /*now the max log size is 8192 KB*/
+	uint16_t *pad_header;
+	block * sched, *last, *cipher_blks, *tag_blks,*cipher_ptr, *tag_ptr;
+	block mask;
+	unsigned char *my_pad, *chr_cnt, *mypad_ptr; /*for padding the last block*/
+	remaining = (uint16_t) msg_len;
+	sched = ((block *)(const_aeskey.rd_key)); //point to AES round keys
+	cipher_blks = (block*)kmalloc(8* sizeof(block), GFP_KERNEL);
+	tag_blks = (block*)kmalloc(3 * sizeof(block), GFP_KERNEL);
+	my_pad = (unsigned char*)kmalloc(16 * sizeof(unsigned char), GFP_KERNEL);
+	last = ((block *)(my_pad)); 
+	chr_cnt = (char *)&counter;
+	pad_header = ((uint16_t*)(my_pad));	
+	tag_blks[2] = _mm_loadu_si128(&current_key);
+	mask =_mm_xor_si128(sched[0], current_key);/*xor the signing key with the aes pre-round key*/
+	
+	mypad_ptr = my_pad;
+	cipher_ptr = cipher_blks;
+	tag_ptr = tag_blks;
+
+	if(remaining>=112)//start 8 blocks parallel computing 
+	{
+		cipher_blks[0]  = _mm_srli_si128(_mm_loadu_si128((block*)log_msg), 2); 
+		cipher_blks[0]  = _mm_insert_epi16(cipher_blks[0], counter+1, 0);
+		gen_7_blks(cipher_blks,log_msg,counter);
+		AES_ECB_8(cipher_blks, sched, mask);
+		tag_8_xor(tag_blks,cipher_blks);
+		counter +=8;
+		log_msg +=110;/*112-byte computed, apply 110-byte, leaving 2-byte overwrote by counter*/
+		remaining -= 112;
+		while(remaining >= 112){
+			cipher_blks[0]  = gen_logging_blk((block*)log_msg, counter+1); 
+			gen_7_blks(cipher_blks,log_msg,counter);
+			AES_ECB_8(cipher_blks, sched, mask);
+			tag_8_xor(tag_blks,cipher_blks);
+			counter +=8;
+			log_msg +=110;
+			remaining -= 112;
+		}
+	}//end of nblks	
+	if(remaining >= 56){//4-block, 4*14=56 bytes log data
+		cmpt_4_blks(cipher_blks,counter, log_msg, sched, mask);
+		tag_blks[0] = xor_block( xor_block(cipher_blks[0], cipher_blks[1]), xor_block(cipher_blks[2], cipher_blks[3])); 
+		tag_blks[2] = xor_block(tag_blks[2], tag_blks[0]);
+		remaining -= 56;
+		counter +=4;
+		log_msg +=54;/*56-byte computed, apply 54-byte, leaving 2-byte overwrote by counter*/
+	}
+	if (remaining >= 28) {//2-block, 2*14=28 bytes log data
+		cmpt_2_blks(cipher_blks,counter, log_msg, sched, mask);
+		tag_blks[2] = xor_block(xor_block(cipher_blks[0], cipher_blks[1]), tag_blks[2]); 
+		remaining -= 28;
+		counter +=2;
+		log_msg +=26;/*28-byte computed, apply 26-byte, leaving 2-byte overwrote by counter*/
+	}
+	if (remaining >= 14) {//1-block 14 bytes log data
+		cmpt_a_blk(&cipher_blks[0],counter, log_msg, sched, mask);
+		tag_blks[2] = xor_block(tag_blks[2], cipher_blks[0]);
+		remaining -= 14;
+		counter +=1;
+		log_msg +=12;/*14-byte computed, apply 12-byte, leaving 2-byte overwrote by counter*/
+	}
+
+	if (remaining){//last block + generating new key&state
+		if (counter)  log_msg +=2;
+		counter += (14-remaining);
+		*last = zero_block();
+		*pad_header = counter;
+		for(i=0;i<remaining;i++) my_pad[i+2]=log_msg[i];
+		cipher_blks[0] = xor_block(*last, mask);
+		cipher_blks[1] = xor_block(current_state, sched[0]);//pre-round [0]
+		cipher_blks[2] = xor_block(cipher_blks[1], _mm_setr_epi32(0x0001, 0x0000, 0x0000, 0x0000));//pre-round [1]
+		AES_ECB_3(cipher_blks, sched);
+		tag_blks[2] = xor_block(tag_blks[2], cipher_blks[0]);
+		current_key = xor_block(cipher_blks[2], current_state);
+		current_state = xor_block(cipher_blks[1], current_state);
+	}else{//generating new key&state
+		cipher_blks[0] = xor_block(current_state, sched[0]);//pre-round [0]
+		cipher_blks[1] = xor_block(cipher_blks[0], _mm_setr_epi32(0x0001, 0x0000, 0x0000, 0x0000));//pre-round [1]
+		AES_ECB_2(cipher_blks, sched);
+		current_key = xor_block(cipher_blks[0], current_state);
+		current_state = xor_block(cipher_blks[1], current_state);
+	}
+	_mm_store_si128((block*)q_tag, tag_blks[2]);
+	//clearing up
+	kfree(mypad_ptr);
+	kfree(cipher_ptr);
+	kfree(tag_ptr);
+}
+#undef gen_7_blks
+#undef prernd_8
+#undef prernd_4
+#undef tag_8_xor
+#undef aes_single
+#undef enc_3
+#undef AES_ECB_3
+#undef enc_4
+#undef AES_ECB_4
+#undef enc_8
+#undef AES_ECB_8
+//-----------------------------------------------------------------
+//endregion
+
+
+
 static void audit_set_portid(struct audit_buffer *ab, __u32 portid)
 {
 	if (ab) {
@@ -1076,7 +1485,11 @@ static void audit_receive(struct sk_buff
 /* Initialize audit support at boot time. */
 static int __init audit_init(void)
 {
-	int i;
+	int i; 
+	/* QuickLog  integrity */
+	block s_0, mask;
+	block init_pair[2];
+	block * sched;
 	struct netlink_kernel_cfg cfg = {
 		.input	= audit_receive,
 	};
@@ -1100,7 +1513,24 @@ static int __init audit_init(void)
 
 	for (i = 0; i < AUDIT_INODE_BUCKETS; i++)
 		INIT_LIST_HEAD(&audit_inode_hash[i]);
+	
+	//---------------------------------------------------------------------------------------
+	/* QuickLog  integrity */
+	get_random_bytes(&s_0, sizeof(block));/*initial State */
+	kernel_fpu_begin();	
+	AES_128_Key_Expansion(aeskey,&const_aeskey); //inital aes round keys
+	init_pair[0] = zero_block();/*0 for updatting state*/
+	init_pair[1] = _mm_setr_epi32(0x0001, 0x0000, 0x0000, 0x0000);/*1 for updatting key*/
+	sched = ((block *)(const_aeskey.rd_key)); 
+	mask =xor_block(s_0, sched[0]);
+	init_pair[0] = xor_block(init_pair[0], mask);
+	init_pair[1] = xor_block(init_pair[1], mask);
+	AES_ECB_2(init_pair, sched);
+	current_state = xor_block(init_pair[0], s_0);
+	current_key = xor_block(init_pair[1], s_0);
+	kernel_fpu_end();
 
+	//---------------------------------------------------------------------------------------
 	return 0;
 }
 __initcall(audit_init);
@@ -1895,20 +2325,40 @@ out:
  */
 void audit_log_end(struct audit_buffer *ab)
 {
+	struct nlmsghdr *nlh;
+	char *log_msg; // log message
+	size_t msg_len;
+    unsigned long flags;
+	
 	if (!ab)
 		return;
 	if (!audit_rate_check()) {
 		audit_log_lost("rate limit exceeded");
 	} else {
-		struct nlmsghdr *nlh = nlmsg_hdr(ab->skb);
-		nlh->nlmsg_len = ab->skb->len - NLMSG_HDRLEN;
+		nlh = nlmsg_hdr(ab->skb);
 
-		if (audit_pid) {
-			skb_queue_tail(&audit_skb_queue, ab->skb);
-			wake_up_interruptible(&kauditd_wait);
-		} else {
-			audit_printk_skb(ab->skb);
-		}
+//------------QuickLog  start -------------
+	log_msg = nlmsg_data(nlh);
+	msg_len = strlen(log_msg);
+		
+/* Perform the cryptographic operations synchronously */
+	spin_lock_irqsave(&(&audit_skb_queue)->lock, flags);
+	kernel_fpu_begin();
+	mac_core(log_msg, msg_len);
+	kernel_fpu_end();
+	audit_log_format(ab, " t=%llx", (unsigned long long)q_tag[0]);
+		
+	nlh->nlmsg_len = ab->skb->len - NLMSG_HDRLEN;
+
+	if (audit_pid) {
+		__skb_queue_tail(&audit_skb_queue, ab->skb);
+		spin_unlock_irqrestore(&(&audit_skb_queue)->lock, flags);
+		wake_up_interruptible(&kauditd_wait);
+	} else {
+		spin_unlock_irqrestore(&(&audit_skb_queue)->lock, flags);
+		audit_printk_skb(ab->skb);
+	}
+//----------------------------------
 		ab->skb = NULL;
 	}
 	audit_buffer_free(ab);
diff -rupN linux-3.10.0-1160.49.1.el7.x86_64/kernel/Makefile aes/kernel/Makefile
--- linux-3.10.0-1160.49.1.el7.x86_64/kernel/Makefile	2021-11-09 08:29:30.000000000 -0500
+++ aes/kernel/Makefile	2022-08-24 21:52:06.890664546 -0400
@@ -1,6 +1,7 @@
 #
 # Makefile for the linux kernel.
 #
+ccflags-y := -Wno-declaration-after-statement -mmmx -maes -mpreferred-stack-boundary=4
 
 obj-y     = fork.o exec_domain.o panic.o printk.o \
 	    cpu.o exit.o itimer.o time.o softirq.o resource.o \
